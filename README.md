# README â€” Ã‰tape 1
IntÃ©gration et transformation de donnÃ©es mÃ©tÃ©orologiques (Forecast 2.0)
Projet : GreenAndCoop â€“ Forecast 2.0

RÃ´le : Data Engineer
Objectif de lâ€™Ã©tape 1 :
Mettre en place un premier pipeline permettant de collecter, transformer, tester et stocker des donnÃ©es mÃ©tÃ©orologiques issues de diffÃ©rentes sources dans Amazon S3, dans un format compatible avec une future intÃ©gration dans MongoDB.

## 1. Contexte du projet

Dans le cadre du projet Forecast 2.0, lâ€™entreprise GreenAndCoop souhaite enrichir ses modÃ¨les de prÃ©vision de la demande Ã©lectrique avec de nouvelles sources de donnÃ©es mÃ©tÃ©orologiques, notamment :

Des stations semi-professionnelles du rÃ©seau InfoClimat

Des stations amateurs du rÃ©seau Weather Underground (France et Belgique)

Ces sources prÃ©sentent :

des formats hÃ©tÃ©rogÃ¨nes (JSON, Excel)

des frÃ©quences diffÃ©rentes

des mÃ©tadonnÃ©es variables

Le rÃ´le du Data Engineer est de construire un pipeline fiable permettant de fournir des donnÃ©es propres, cohÃ©rentes et exploitables par les Data Scientists.

## 2. Architecture de lâ€™Ã©tape 1
Vue dâ€™ensemble

```java
Sources mÃ©tÃ©o
   â”‚
   â–¼
Airbyte
   â”‚
   â–¼
Amazon S3 (zone RAW)
   â”‚
   â–¼
Scripts Python (transformation + tests)
   â”‚
   â–¼
Amazon S3 (zone PROCESSED, format MongoDB-ready)
```
Structure S3 utilisÃ©e

```pgsql
s3://p8-meteo/
â”‚
â”œâ”€â”€ p8-data-path/                # DonnÃ©es brutes (Airbyte)
â”‚   â”œâ”€â”€ InfoClimat_data/
â”‚   â”œâ”€â”€ France_data/
â”‚   â””â”€â”€ Belgique_data/
â”‚
â””â”€â”€ p8-processed/                # DonnÃ©es transformÃ©es
    â”œâ”€â”€ weather_mongodb_ready.json
    â””â”€â”€ quality_report.json
```

## 3. Collecte des donnÃ©es (Airbyte)

La collecte est rÃ©alisÃ©e avec Airbyte, qui permet de :

se connecter aux diffÃ©rentes sources mÃ©tÃ©o

uniformiser lâ€™extraction

stocker les donnÃ©es brutes dans Amazon S3

Les donnÃ©es sont exportÃ©es au format JSONL dans un bucket S3.

ğŸ‘‰ Aucune transformation nâ€™est faite dans Airbyte
Toute la logique mÃ©tier est volontairement gÃ©rÃ©e cÃ´tÃ© Python.

### 3.1 Installation et configuration dâ€™Airbyte
PrÃ©requis :

Docker Desktop installÃ© et en fonctionnement

Terminal (PowerShell, CMD ou bash)

Installation Airbyte via Docker

Suivre les Ã©tapes de la documentation officiel:
https://docs.airbyte.com/platform/using-airbyte/getting-started/oss-quickstart

AccÃ©der Ã  Airbyte :
Ouvrir un navigateur â†’ http://localhost:8000

### 3.2 PrÃ©paration AWS
    #### 3.2.1 CrÃ©ation dâ€™un bucket S3

Connectez-vous Ã  votre console AWS.

Allez dans S3 â†’ Create bucket

Donnez un nom unique, ex : p8-meteo

Configurez la rÃ©gion (ex : eu-west-3)

Laissez les autres paramÃ¨tres par dÃ©faut â†’ Create bucket

    #### 3.2.2 CrÃ©ation dâ€™un utilisateur IAM pour Airbyte

Aller dans IAM â†’ Users â†’ Add user

Nom : airbyte-s3-user

AccÃ¨s programmatique â†’ cochez Programmatic access

Attachez la policy AmazonS3FullAccess

Cliquez sur Create user

RÃ©cupÃ©rez :

AWS_ACCESS_KEY_ID

AWS_SECRET_ACCESS_KEY

âš ï¸ Important : Conservez-les prÃ©cieusement, ne les mettez pas dans Git.

### 3.3 Connexion Airbyte â†’ AWS S3 (Destination)

Dans Airbyte :

Ajouter une destination

Type : Amazon S3

Renseignez :

Access Key ID â†’ AWS_ACCESS_KEY_ID

Secret Access Key â†’ AWS_SECRET_ACCESS_KEY

Bucket Name â†’ p8-meteo

Region â†’ eu-west-3

Testez la connexion â†’ Save

### 3.4 Configuration Airbyte
    DÃ©finition des sources et de la destination dans Airbyte
#### 3.4.1 DÃ©finir les sources mÃ©tÃ©o

Dans Airbyte :

Ajouter une source â†’ choisir le connecteur correspondant :

InfoClimat : JSON depuis API ou fichiers bruts

Weather Underground : Excel ou API selon le setup

Renseigner les paramÃ¨tres spÃ©cifiques Ã  chaque source :

Nom de la source (ex : InfoClimat_FR)

Chemin dâ€™accÃ¨s ou URL

FrÃ©quence de synchronisation

Tester la connexion â†’ Save

RÃ©pÃ©tez pour toutes les sources (InfoClimat, France_data, Belgique_data).

#### 3.4.2 DÃ©finir la destination S3

Ajouter une destination â†’ choisir Amazon S3

ParamÃ¨tres Ã  remplir :

AWS Access Key ID â†’ AWS_ACCESS_KEY_ID

AWS Secret Access Key â†’ AWS_SECRET_ACCESS_KEY

Bucket Name â†’ p8-meteo

Region â†’ eu-west-3

Format â†’ JSONL

Tester la connexion â†’ Save

#### 3.4.3 CrÃ©er la connexion (Sync) Airbyte

Aller dans Connections â†’ New connection

SÃ©lectionner :

Source â†’ la source mÃ©tÃ©o dÃ©finie

Destination â†’ Amazon S3

Configurer :

FrÃ©quence â†’ Ex. Every hour ou Manual

Mode de chargement â†’ Overwrite ou Append selon le besoin

Tester â†’ Save â†’ Sync now

Les donnÃ©es brutes de chaque source seront automatiquement stockÃ©es dans S3 (p8-data-path/â€¦) en JSONL, prÃªtes pour la transformation Python.


## 4. Transformation des donnÃ©es (S3 â†’ S3)
Objectif

Transformer les donnÃ©es brutes en documents JSON compatibles MongoDB, avec :

un schÃ©ma commun entre toutes les sources

la conservation maximale des informations

une structure facilement requÃªtable

SchÃ©ma cible (logique)
```json
{
  "source": "weather_underground | infoclimat",
  "station": { ... },
  "timestamp": "ISO-8601",
  "measurements": { ... }
}
```bash

Points techniques importants
Weather Underground

Les fichiers Excel contenaient une feuille par jour

La date nâ€™est pas conservÃ©e par Airbyte

Les donnÃ©es sont ordonnÃ©es par heure (Time)

ğŸ‘‰ La date est reconstruite en dÃ©tectant le retour Ã  00:xx:xx, ce qui indique un changement de jour.

InfoClimat

Les mÃ©tadonnÃ©es des stations sont extraites et intÃ©grÃ©es

Toutes les mesures disponibles sont conservÃ©es (tempÃ©rature, pression, pluie, neige, vent, etc.)

Les valeurs manquantes sont gÃ©rÃ©es explicitement (None ou 0 selon le cas)


## 5. ContrÃ´le qualitÃ© des donnÃ©es

Un script dÃ©diÃ© permet de mesurer la qualitÃ© des donnÃ©es aprÃ¨s transformation :

VÃ©rifications effectuÃ©es

PrÃ©sence de valeurs manquantes critiques (tempÃ©rature)

DÃ©tection de doublons (station_id + timestamp)

Calcul dâ€™un taux dâ€™erreur global

```yaml
RÃ©sultat obtenu
Total records        : 4950
Valeurs manquantes   : 0
Doublons             : 0
Taux dâ€™erreur        : 0.0 %
```

Ces rÃ©sultats garantissent que les donnÃ©es sont prÃªtes pour une intÃ©gration en base NoSQL.

## 6. PrÃ©requis techniques
Environnement

Windows

Python â‰¥ 3.9

Compte AWS actif

Docker Desktop installÃ© (utilisÃ© dans les Ã©tapes suivantes)

## 7. Configuration AWS
Installation de lâ€™AWS CLI

TÃ©lÃ©charger et installer lâ€™AWS CLI depuis :
https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html

Configuration des credentials

Dans un terminal PowerShell :
```bash
aws configure
```

Renseigner :
```bash
AWS Access Key ID

AWS Secret Access Key

Default region name (ex : eu-west-3)

Default output format : json
```

VÃ©rification
```bash
aws s3 ls
```

Le bucket p8-meteo doit apparaÃ®tre.

## 8. DÃ©pendances Python
requirements.txt
```bash
boto3==1.42.19
python-dateutil==2.9.0
```bash

Installation

Avec un environnement virtuel (recommandÃ©) :

```bash
pip install -r requirements.txt
```

## 9. ExÃ©cution des scripts
Transformation des donnÃ©es
```bash
python transform_weather_s3.py
```

RÃ©sultat :
```bash
p8-processed/weather_mongodb_ready.json
```

Tests de qualitÃ©
```bash
python data_quality_checks_s3.py
```

RÃ©sultat :
```bash
p8-processed/quality_report.json
```
## 10. Conclusion de lâ€™Ã©tape 1

Ã€ lâ€™issue de cette Ã©tape :

Les donnÃ©es sont centralisÃ©es dans S3

Elles sont nettoyÃ©es, structurÃ©es et enrichies

Leur qualitÃ© est mesurÃ©e et validÃ©e

Le format est directement compatible MongoDB

ğŸ‘‰ Le pipeline est prÃªt pour lâ€™Ã‰tape 2 : intÃ©gration dans MongoDB, mise en place du schÃ©ma et des collections.